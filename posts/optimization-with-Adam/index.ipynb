{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimiziation with Adam \n",
    "author: Eduardo Pareja Lema\n",
    "date: '2023-05-14'\n",
    "image: \"Adam.png\" \n",
    "description: \"A blog post on the implementation of the Adam optimizer\"\n",
    "format: html \n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "\n",
    "In this blog post we implement three different optimization techniques for empirical risk minimization on the logistic loss: regular gradient descent, stochastic gradient descent, and Adam. The latter is a state-of-the-art optimization algorithm widely used in modern deep learning. \n",
    "\n",
    "To code for all the implementations described above can be found at: \n",
    "\n",
    "<>\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Experiments \n",
    "\n",
    "We perform similar experiment as we did on our previous blog post on logistic regression: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimen 1 \n",
    "\n",
    "In this experiment "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
