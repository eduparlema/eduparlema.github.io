[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Link to Code\nhttps://github.com/eduparlema/eduparlema.github.io/blob/main/posts/kernel-logistic-regression/kernel_logistic.py\n\n\nBackground Information and Implementation\nIn kernel logistic regression we still perform Empirical Risk Minimization but with a modified loss function: \\[L_k(v) = \\frac{1}{n} \\sum_{i=1}^n l(\\langle v \\; , \\; k(x_i) \\rangle, y_i)\\] where \\(v \\in \\mathbb{R}^n\\) and \\(k(x_i)\\) is a modified feature vector dependent on a kernel function.\nIn order to implement this, we make use of the minimize function from scipy.optimize. Similarly, we start by padding \\(X\\) and initializing a random vector \\(v\\) (in this case of size \\(n\\)). Then, we compute the modified feature vector. To calculate the empirical risk we simply find the logistic loss of a matrix multiplication between the modified feature vector and \\(v\\). Note that we are able to do this since the predictor function is still and inner product.\n\nimport numpy as np \nfrom scipy.optimize import minimize\n\ndef sigmoid(self, z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\ndef fit(self, X, y):\n        self.X_train = X\n        X_ = self.pad(X)\n\n        v0 = np.random.rand(X.shape[0])\n        km = self.kernel(X_, X_, **self.kernel_kwargs)\n        \n        def empirical_risk(km, y, v, loss):\n            y_hat = km@v\n            return loss(y_hat, y).mean()\n        \n        result = minimize(lambda v: empirical_risk(km, y, v, self.logistic_loss), x0 = v0) \n        self.v = result.x\n\n\n\nBasic Checks\nIf we test our implementation, we find that the model is able to handle non-linear decision boundaries successfully. Here, the “rbf_kernel” is the kernel function and “gamma” is a parameter to that kernel function that says how wiggly the decision boundary should be. In other words, a large gamma should result in an overfitted model.\n\n%reload_ext autoreload \n%autoreload 2\n\nfrom kernel_logistic import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, as predicted before, a large gamma results in an overfitted model:\n\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment\nLets investigate which value of gamma is best for our model by plotting a graph of the training and validation score against different gamma values:\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed()\n\ndef experiment(noise, data_geometry):\n    gamma_values = 10.0**np.arange(-1, 7)\n    df = pd.DataFrame({\"gamma\": [], \"train\" : [], \"test\" : []})\n\n    for _ in range(10): #we perform 10 runs of the experiment and take the mean \n        X_train, y_train = data_geometry(100, shuffle = True, noise = noise)\n        X_test, y_test = data_geometry(100, shuffle = True, noise = noise)\n\n        for gamma in gamma_values:\n            KLR = KernelLogisticRegression(rbf_kernel, gamma = gamma)\n            KLR.fit(X_train, y_train)\n            to_add = pd.DataFrame({\"gamma\" : [gamma],\n                                \"train\" : [KLR.score(X_train, y_train)],\n                                \"test\" : [KLR.score(X_test, y_test)]})\n\n            df = pd.concat((df, to_add))\n\n\n    means = df.groupby(\"gamma\").mean().reset_index()\n\n    plt.xscale(\"log\")\n    plt.plot(means[\"gamma\"], means[\"train\"], label = \"training\")\n    plt.plot(means[\"gamma\"], means[\"test\"], label = \"validation\")\n    plt.legend()\n    labs = plt.gca().set(xlabel = \"Value of gamma\",\n                ylabel = \"Accuracy\") \n\nexperiment(0.2, make_moons)\n\n\n\n\nFrom here, we find that a gamma of around 100 is best for this particular model. Now, what if we vary the noise of the data? Intuitively, a low noise should bring the training and validation scores “closer” since the pattern of the training data would not differ significantly from the testing data. On the other hand, higher noise should result in lower accuracy and it should take bigger values of gamma for the training score to converge.\n\nexperiment(0.1, make_moons)\n\n\n\n\n\nexperiment(0.4, make_moons)\n\n\n\n\nWhile the results fit our predictions, these graphs suggest that the value of gamma for each model is still around 100. In other words, gamma is independent from the noise!\n\n\nA Different Data Pattern\nFinally we try a different pattern to see the results.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nexperiment(0.1, make_circles)\n\n\n\n\n\nexperiment(0.2, make_circles)\n\n\n\n\nHere, we still find that the best gamma value is independent of the noise! However, such gamma value is 1 in this case, which differs from the previous pattern."
  },
  {
    "objectID": "posts/perceptron /index.html",
    "href": "posts/perceptron /index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "The goal of this blog post is to implement the perceptron, a classic binary linear classifier algorithm. We implement such algorithm in Python, try some experiments and showcase its limitations with data that is not linearly separable. Moreover, at the end we perform a complexity analysis of this algorithm."
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-1",
    "href": "posts/perceptron /index.html#experiment-1",
    "title": "The Perceptron",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we use 2d data to show that our perceptron algorithm converges to a weight vector \\(\\tilde{w}\\) describing a separating line as shown below.\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-2",
    "href": "posts/perceptron /index.html#experiment-2",
    "title": "The Perceptron",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we also use 2d data, but not linearly separable, so that the perceptron algorithm will not converge to a weight vector \\(\\tilde{w}\\) that describes a separating line. It is possible to see that in the accuracy graph, the perceptron algorihtm never reaches a perfect accuracy\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-3",
    "href": "posts/perceptron /index.html#experiment-3",
    "title": "The Perceptron",
    "section": "Experiment 3",
    "text": "Experiment 3\nIn this experiment we try to apply the perceptron algorithm in more than 2 dimensions. Here we try 5 features:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\nBased on the graph above, it does not seem that the data is linearly separable, as the accuracy does not seem to improve with the number of iterations making it unlikely that it will converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement simple gradient descent, stochastic gradient descent, and a momentum method. These are all helpful algorithms to optimize convex functions given that finding any local minimizer is equivalent to finding the global minimizer for these kind of functions. Then, we compare the performance of these three algorithms for training logistic regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-1",
    "href": "posts/logistic-regression/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we will explore how the outcome of the gradient descent changes depending on the value of the learning rate alpha.\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs=1000)\n\n#Graph the data with a separator\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.show()\n\n\n#Graph the gradient and score history \nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\nnum_steps = len(LR.loss_history)\n\naxarr[0].plot(np.arange(num_steps) + 1, LR.loss_history)\nlabs = axarr[0].set(title=\"Gradient history\", xlabel=\"Epochs\", ylabel=\"Gradient\")\naxarr[1].plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\nlabs = axarr[1].set(title=\"Accuracy history\", xlabel=\"Epochs\", ylabel=\"Accuracy\")\n\n\n\n\n\n\n\nWe first see that the algorithm works pretty well. Even though the data is not linearly separable, unlike the perceptron, the gradient descent algorithm is able to accurately fit the weights to the data to find an accurate linear separator. We are also able to see how the accuracy improves until almost reaching 1. Now, lets try chaning alpha and see how it affects the gradien descent.\n\nLR_a = LogisticRegression()\nLR_a.fit(X, y, alpha = 10, max_epochs=1000)\n\nnum_steps = len(LR_a.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR_a.loss_history)\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\nplt.title(\"alpha = 10\")\nplt.show()\n\nLR_A = LogisticRegression()\nLR_A.fit(X, y, alpha = 50, max_epochs=1000)\n\nnum_steps = len(LR_A.loss_history)\nloss_fig1 = plt.plot(np.arange(num_steps) + 1, LR_A.loss_history)\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\nplt.title(\"alpha = 50\")\nplt.show()\n\n\n\n\n\n\n\nWe can see in the first case that the gradient descent algorithm still converges but a little slower than with a small alpha. If we increase alpha to 50, however, the algorithm clearly does not converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-2",
    "href": "posts/logistic-regression/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we will explore how the size of the batches influences how quickly the stochastic gradient descent algorithm converges.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 20, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is possible to see how the size of the batch affets the convergence. In fact, it seems that a smaller size of the batch leads to a faster convergence. Intuitively, choosing a big size for the batch defeats the purpose of stochastic gradient descent, so it makes sense that a smaller size leads to a fast convergence."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-3",
    "href": "posts/logistic-regression/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nNow, we explore how the momentum method enhances the stochastic gradient descent.\n\n#Graph stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#Graph stochastic gradient descent with momentum \nLR_m = LogisticRegression()\nLR_m.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR_m.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_m.loss_history, label = \"stochastic gradient (momentum)\")\n\nplt.loglog()\nlegend = plt.legend() \n\n\n\n\nEven though there is some noice once it reaches convergence, the momentum method clearly speeds up convergence significantly."
  },
  {
    "objectID": "posts/unsupervised-learning/index.html",
    "href": "posts/unsupervised-learning/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Image Compression with SVD\nIn this part of the blog post, we explore Single Value Decomposition for image compression.\nA SVD of a real matrix \\(A \\in \\mathbb{R}^{m \\times n }\\) is \\[\\begin{equation}\n    A = UDV^T ,\n\\end{equation}\\] where \\(D \\in \\mathbb{R}^{m \\times n }\\) only has non-zero entries in its diagonal. This non-zero values are the singular values \\(\\sigma_i\\). The other matrices \\(U \\in \\mathbb{R}^{m \\times m}\\) and \\(V \\in \\mathbb{R}^{n \\times n }\\) are orthogonal matrices. One of the reasons SVD is useful is that we can approximate some matrix \\(A\\) using a much smaller representation. To do this, we can pick only: - The first \\(k\\) columns of \\(U\\). - The top \\(k\\) singular values of \\(D\\). - The first \\(k\\) rows of \\(V\\).\nThis is exactly what we will be attempting in this blog post. For this, we exctract an image from the internet and to simplify things, let us convert it to grey scale:\n\nimport PIL\nimport urllib\nimport numpy as np \nimport matplotlib.pyplot as plt \n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nurl = \"https://cdn.shopify.com/s/files/1/0163/6622/articles/Landscape_Photo_Tips_1024x.jpg?v=1660006689\"\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nWe can now write a method that finds the SVD decomposition of the image allow us to approximate it using \\(k\\) singular values. We use a a method from numpy linalg.svd to do this for us.\n\ndef svd_reconstruct(img, k):\n    # compute the singular value decomposition (SVD) \n    U, sigma, V = np.linalg.svd(img)\n\n    # create the D matrix in the SVD \n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n\n    # Approximate img using SVD \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    A_ = U_ @ D_ @ V_ \n    return A_ \n\nLet us now write an experiment to see how well the approximation is as \\(k\\) increases. Also, note that in an \\(m\\times n\\) grey scale image, we need \\(mn\\) pixels to represent it. However, in SVD, we now that \\(U\\in \\mathbb{R}^{m\\times k}\\), \\(D\\in \\mathbb{R}^{k\\times k}\\), and \\(U\\in \\mathbb{R}^{k\\times n}\\), so that we only need \\(mk + kk + nk\\) pixels to represent the image.\n\ndef svd_experiment(img, rows=3, cols= 4, increment = 10):\n    fig ,axarr = plt.subplots(rows, cols, figsize=(20,10))\n    for i, ax in enumerate(axarr.flatten()): \n        components  = increment + increment*i\n        img_ = svd_reconstruct(img, components)\n        ax.imshow(img_, cmap='Greys')\n        ax.axis('off')\n\n        # Compute the storage \n        original_storage = img.shape[0] * img.shape[1]\n        new_storage = img.shape[0]*components + components**2 + components*img.shape[1]\n\n        ax.set(title = f\"{components} components, % storage = {round(new_storage/original_storage * 100, 2)} \")\n\n        # Show the original image at the end to compare results \n        if i == rows*cols - 1: \n            ax.imshow(grey_img, cmap='Greys')\n            ax.axis('off')\n            ax.set(title = \"Original image\")\n\nsvd_experiment(grey_img)\n\n\n\n\nWe can see that already at around 40-50 components we already have a pretty good approximation of the image and we are only using around 12% of the original storage, remarkable!\nWe can add an extra parameter to our svd_reconstruct function so that we can reconstruct an image to a desired compression factor instead of a \\(k\\). To do this, suppose we have a compression factor of \\(\\alpha\\). Then we solve for \\(k\\) in the following equation:\n\\[mk + k^2 + nk = \\frac{\\alpha mn}{100}\\]\nThis is simply a quadratic equation. One of its roots is negative so in the function below we just take the positive sign for the square root. Once we solve for \\(k\\) we simply take its floor function since it must be an integer.\n\nfrom math import sqrt, floor\ndef svd_reconstruct(img, factor):\n    # compute the singular value decomposition (SVD) \n    U, sigma, V = np.linalg.svd(img)\n\n    # create the D matrix in the SVD \n    D = np.zeros_like(img, dtype=float)\n    D[:min(img.shape), :min(img.shape)] = np.diag(sigma)\n    \n    m, n = img.shape\n    k = (-(m+n)+sqrt((m+n)**2 + (4*factor*m*n) / (100) )) / 2\n    k = floor(k)\n    # Approximate img using SVD \n    U_ = U[:, :k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    A_ = U_ @ D_ @ V_ \n    return A_ \n\nfactor = 20\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\nimg_ = svd_reconstruct(grey_img, factor)\n\naxarr[0].imshow(grey_img, cmap = \"Greys\")\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original image\")\n\naxarr[1].imshow(img_, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = f\"image compressed by {factor}%\")\n\n[Text(0.5, 1.0, 'image compressed by 20%')]"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Demo\nTo test our implementation, first let us write a function that produces data for us and let us visualize it:\n\n%reload_ext autoreload \n%autoreload 2\n\nimport numpy as np\nfrom linearRegression import LinearRegression\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nNow, we print the scores and draw the lines from the resulting weights of both models implemented:\n\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score (Analytical) = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score (Analytical) = {LR.score(X_val, y_val).round(4)}\")\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.0005) \n\nprint(f\"Training score (Gradient)= {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score (Gradient) = {LR2.score(X_val, y_val).round(4)}\")\n\n#Plot both analytical and gradient descent results \nx = np.linspace(0, 1, 101)\ny_analytical = (LR.w[0]*x + LR.w[1])\ny_gradient = (LR2.w[0]*x + LR2.w[1])\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\n#print lines\naxarr[0].plot(x, y_analytical, color=\"black\", label=\"analytical\")\naxarr[1].plot(x, y_analytical, color=\"black\", label=\"analytical\")\naxarr[0].plot(x, y_gradient, color=\"red\", label=\"gradient\")\naxarr[1].plot(x, y_gradient, color=\"red\", label=\"gradient\")\naxarr[0].legend(loc=\"best\")\naxarr[1].legend(loc=\"best\")\n\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\nTraining score (Analytical) = 0.6114\nValidation score (Analytical) = 0.6243\nTraining score (Gradient)= 0.5788\nValidation score (Gradient) = 0.6177\n\n\n\n\n\nWe see how both implementations lead to the similar results. The analytical formula seems to me slightly better, this is probably due to the learning rate of the gradient descent or the number of iterations.\nNow, to make sure the implementation of gradient descent is correct, let us visualize the how the score changed over time. The score should increase monotonically, and it does:\n\nplt.plot(LR2.score_history)\n\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.show()\n\n\n\n\n\n\nExperiment\nWhat if we increaase the number of features? Our model would have more data points so we should expect to produce a better training score as the number of features increase. However, would this lead to overfitting? Well, let’s perform an experiment that allow us to visualize the change in both the trainig and validation score.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnumber_features = []  \nvalidation_score = [] \ntraining_score = []\n\nLR = LinearRegression()\n\nfor p_features in range(1, n_val):\n    number_features.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit_analytical(X_train, y_train)\n    validation_score.append(LR.score(X_val, y_val))\n    training_score.append(LR.score(X_train, y_train))\n\nplt.plot(validation_score, label = \"validation\")\nplt.plot(training_score, label = \"training\")\nplt.legend(loc='best')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\nplt.show()\n    \n\n\n\n\nJust as predicted, the training score seems to approach 1 as the numer of features increase. However, the validation score clearly decreases after some point, which suggests that increasing the number of features leads to overfitting.\nWe now run the same experiment but with the LASSO algorithm. This algorithm uses a modified loss function with a regularization term \\(\\alpha\\):\n\\[L(\\vec{w}) = || Xw - y||^2_2 + \\alpha ||\\vec{w}'||_1\\]\nLet’s visualize the experiment on LASSO with varying regularization strengths:\n\n%%capture --no-display\nfrom sklearn.linear_model import Lasso\n\nnumber_features = []  \nvalidation_score_1 = [] \ntraining_score_1 = []\nvalidation_score_2 = [] \ntraining_score_2 = []\nvalidation_score_3 = [] \ntraining_score_3 = []\n\nL1 = Lasso(alpha=0.01)\nL2 = Lasso(alpha=0.001)\nL3 = Lasso(alpha=0.0001)\n\nfor p_features in range(1, n_val+5):\n    number_features.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L1.fit(X_train, y_train)\n    validation_score_1.append(L1.score(X_val, y_val))\n    training_score_1.append(L1.score(X_train, y_train))\n    L2.fit(X_train, y_train)\n    validation_score_2.append(L2.score(X_val, y_val))\n    training_score_2.append(L2.score(X_train, y_train))\n    L3.fit(X_train, y_train)\n    validation_score_3.append(L3.score(X_val, y_val))\n    training_score_3.append(L3.score(X_train, y_train))\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\n\naxarr[0].plot(validation_score_1, label = \"validation\")\naxarr[0].plot(training_score_1, label = \"training\")\naxarr[0].legend(loc='best')\n\naxarr[1].plot(validation_score_2, label = \"validation\")\naxarr[1].plot(training_score_2, label = \"training\")\naxarr[1].legend(loc='best')\n\naxarr[2].plot(validation_score_3, label = \"validation\")\naxarr[2].plot(training_score_3, label = \"training\")\naxarr[2].legend(loc='best')\n\nlabs = axarr[0].set(title = \"alpha = 0.01\", xlabel = \"Features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"alpha = 0.001\", xlabel = \"Features\")\nlabs = axarr[2].set(title = \"alpha = 0.0001\", xlabel = \"Features\")\nfig.set_size_inches(9, 4, forward=True)\n\n\n\n\nWe observe that the LASSO algorithm reaches a better accuracy than the regular linear regression, as it quickly gets close to 1. However, we observe the same overfitting trend. The regularization strenght seems to affect how the validation score drops, a smaller alpha seems to be better for overfitted models."
  },
  {
    "objectID": "posts/auditing-allocative-bias/index.html",
    "href": "posts/auditing-allocative-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "The Problem\nIn this blog post we will predict whether an individual’s income in the state of California is over $50k based on demographic characteristics (excluding sex), and then audit for gender bias.\nWe consider the following demographic characteristics along with their respective feature code: Age (AGEP), Class of worker (COW), Educational Attainment (SCHL), Marital Status (MAR), Occupation (OCCP), Relationship (RELP), Usual hours worked per week past 12 months (WKHP), Recoded detailed race code (RAC1P), Disbility code (DIS), Employment status of parents (ESP), and Citizenship Status (CIT).\nLet’s download the data and import some dependencies:\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000004\n      9\n      1\n      3701\n      4\n      6\n      1013097\n      32\n      30\n      ...\n      34\n      60\n      60\n      7\n      8\n      59\n      33\n      8\n      58\n      32\n    \n    \n      1\n      P\n      2018GQ0000013\n      9\n      1\n      7306\n      4\n      6\n      1013097\n      45\n      18\n      ...\n      0\n      0\n      0\n      91\n      46\n      46\n      0\n      89\n      45\n      0\n    \n    \n      2\n      P\n      2018GQ0000016\n      9\n      1\n      3755\n      4\n      6\n      1013097\n      109\n      69\n      ...\n      105\n      232\n      226\n      110\n      114\n      217\n      2\n      111\n      2\n      106\n    \n    \n      3\n      P\n      2018GQ0000020\n      9\n      1\n      7319\n      4\n      6\n      1013097\n      34\n      25\n      ...\n      67\n      0\n      34\n      34\n      69\n      0\n      34\n      35\n      0\n      0\n    \n    \n      4\n      P\n      2018GQ0000027\n      9\n      1\n      6511\n      4\n      6\n      1013097\n      46\n      31\n      ...\n      47\n      81\n      10\n      11\n      79\n      47\n      44\n      81\n      47\n      10\n    \n  \n\n5 rows × 286 columns\n\n\n\nWe now filter the features we will use for the problem stated above:\n\nfeatures_to_use = ['AGEP', 'COW','SCHL', 'MAR', 'OCCP', 'RELP', 'WKHP', 'RAC1P', 'DIS', 'ESP', 'CIT']\nacs_data[features_to_use].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      RELP\n      WKHP\n      RAC1P\n      DIS\n      ESP\n      CIT\n    \n  \n  \n    \n      0\n      30\n      6.0\n      14.0\n      1\n      9610.0\n      16\n      40.0\n      8\n      2\n      NaN\n      1\n    \n    \n      1\n      18\n      NaN\n      14.0\n      5\n      NaN\n      17\n      NaN\n      1\n      2\n      NaN\n      1\n    \n    \n      2\n      69\n      NaN\n      17.0\n      1\n      NaN\n      17\n      NaN\n      9\n      1\n      NaN\n      1\n    \n    \n      3\n      25\n      NaN\n      1.0\n      5\n      NaN\n      17\n      NaN\n      1\n      1\n      NaN\n      1\n    \n    \n      4\n      31\n      NaN\n      18.0\n      5\n      NaN\n      16\n      NaN\n      1\n      2\n      NaN\n      1\n    \n  \n\n\n\n\nWe now construct a BasicProblem, which expresses our desire to use the features above to predict if an individual has a salary greater than $50k (PINCP) using sex (SEX) as the group label. Note that a 1 represents a Male and 2 a Female.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    #Transform the column to check if salary > 50k \n    target_transform=lambda x: x > 50000,\n    group='SEX',\n    preprocess=adult_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nWe can now prepare our data for training (and testing):\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\n#Perform a train-test split \nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\nBasic Description of Training Data\nBefore we train our model, let’s explore our data using a DataFrame.\n\nimport pandas as pd \ndf = pd.DataFrame(X_train, columns=features_to_use)\n# Add a column for the group (1 for male, 2 for female)\ndf['SEX'] = group_train\n# Add a column for the label, (True if >50k, False else)\ndf['SALARY>50k'] = y_train\ndf.head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      RELP\n      WKHP\n      RAC1P\n      DIS\n      ESP\n      CIT\n      SEX\n      SALARY>50k\n    \n  \n  \n    \n      0\n      46.0\n      2.0\n      22.0\n      1.0\n      1821.0\n      0.0\n      45.0\n      9.0\n      2.0\n      0.0\n      1.0\n      2\n      True\n    \n    \n      1\n      45.0\n      1.0\n      21.0\n      3.0\n      4850.0\n      13.0\n      50.0\n      1.0\n      2.0\n      0.0\n      1.0\n      2\n      True\n    \n    \n      2\n      40.0\n      1.0\n      21.0\n      5.0\n      1021.0\n      5.0\n      40.0\n      6.0\n      2.0\n      0.0\n      4.0\n      2\n      True\n    \n    \n      3\n      59.0\n      1.0\n      24.0\n      1.0\n      300.0\n      0.0\n      40.0\n      6.0\n      2.0\n      0.0\n      4.0\n      1\n      True\n    \n    \n      4\n      23.0\n      1.0\n      19.0\n      5.0\n      3401.0\n      11.0\n      40.0\n      1.0\n      2.0\n      0.0\n      1.0\n      2\n      False\n    \n  \n\n\n\n\n\ndf.shape\n\n(156532, 13)\n\n\nBased on the dimension of our DataFrame, our training data has information for about 156532 individuals (and 13 features for each).\nOf these individuals, 41.08% have a salary higher than $50K has showed in the following table:\n\nhigh_salary = df.groupby('SALARY>50k').size().reset_index(name='count')\nhigh_salary['ratio'] = (high_salary[\"count\"] / df.shape[0] * 100).round(2)\nhigh_salary\n\n\n\n\n\n  \n    \n      \n      SALARY>50k\n      count\n      ratio\n    \n  \n  \n    \n      0\n      False\n      92232\n      58.92\n    \n    \n      1\n      True\n      64300\n      41.08\n    \n  \n\n\n\n\nOf those individuals with a salary greater than $50K, roughly 59.83% are male while 40.17% are female.\n\n# Filter only those individuals with salary greater than $50k\nfilter_by_salary = df[df['SALARY>50k']==True]\nhigh_salary_sex = filter_by_salary.groupby('SEX').size().reset_index(name='count')\nhigh_salary_sex['ratio'] = (high_salary_sex['count'] / filter_by_salary.shape[0] * 100).round(2)\nhigh_salary_sex\n\n\n\n\n\n  \n    \n      \n      SEX\n      count\n      ratio\n    \n  \n  \n    \n      0\n      1\n      38472\n      59.83\n    \n    \n      1\n      2\n      25828\n      40.17\n    \n  \n\n\n\n\nSimilarly, we find that based on the tables below, 47% of male individuals and 37% of female individuals have an income greater than $50K.\n\n# Filter male\nfilter_by_male = df[df['SEX'] == 1]\nmale_high_salary = filter_by_male.groupby('SALARY>50k').size().reset_index(name='count')\nmale_high_salary['ratio'] = (male_high_salary['count'] / filter_by_male.shape[0] * 100).round(2)\nmale_high_salary\n\n\n\n\n\n  \n    \n      \n      SALARY>50k\n      count\n      ratio\n    \n  \n  \n    \n      0\n      False\n      44218\n      53.47\n    \n    \n      1\n      True\n      38472\n      46.53\n    \n  \n\n\n\n\n\n# Filter female \nfilter_by_female = df[df['SEX'] == 2]\nfemale_high_salary = filter_by_female.groupby('SALARY>50k').size().reset_index(name='count')\nfemale_high_salary['ratio'] = (female_high_salary['count'] / filter_by_female.shape[0] * 100).round(2)\nfemale_high_salary\n\n\n\n\n\n  \n    \n      \n      SALARY>50k\n      count\n      ratio\n    \n  \n  \n    \n      0\n      False\n      48014\n      65.02\n    \n    \n      1\n      True\n      25828\n      34.98\n    \n  \n\n\n\n\nIt is also possible to investigate intersectionality trends in our data by studying the proportion of positive target labels broken down by SEX and an additional group label. In this case, let us investigate the intersectionality trends between SEX and RAC1P.\nRecall that RAC1P stands for the race of the individuals (1 for White Alone, 2 for Black/African American alone, 3 for Native American alone, 4 for Alaska Native alone, 5 for Native American and Alaska Native tribes specified, 6 for Asian alone, 7 for Native Hawaiian and Other Pacific Islander alone, 8 for Some Other Race alone, 9 for two or more races).\nLet’s create a new DataFrame holding the desired information and then we visualize our findings using the seaborn package.\n\nimport seaborn as sns\n\n# Filter those individuals with salary greater than $50k\nfilter_by_salary = df[df['SALARY>50k'] == True]\n\n# Create new data frame \nintersectionality = filter_by_salary.groupby(['SEX', 'RAC1P']).size().reset_index(name='count')\nintersectionality['ratio'] = (intersectionality['count'] / filter_by_salary.shape[0] * 100).round(2)\n\n# Visualize data\nsns.barplot(data=intersectionality, x = \"RAC1P\", y='ratio', hue='SEX').set(title= 'Individualals with salary > $50K by sex and race')\n\n[Text(0.5, 1.0, 'Individualals with salary > $50K by sex and race')]\n\n\n\n\n\nAccording to the data above, white male individuals make up the majority of those who earn a salary greater than $50K, followed by white woman.\n\n\nTraining the Model\nNow let’s train the model. We will use logistic regression with polynomial features. Let us first find which polynomial degree performs best:\n\nimport warnings; warnings.simplefilter('ignore')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#Constructs a model with polynomial features \ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = int(1e3)))])\n\n\nfor deg in range(5):\n  plr = poly_LR(deg = deg)\n  cv_scores = cross_val_score(plr, X_train, y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Polynomial degree = {deg}, score = {mean_score.round(3)}\")\n\nPolynomial degree = 0, score = 0.589\nPolynomial degree = 1, score = 0.77\nPolynomial degree = 2, score = 0.759\nPolynomial degree = 3, score = 0.73\nPolynomial degree = 4, score = 0.658\n\n\nIt seems that a degree-1 polynomial works best according to our cross-validation. So let us create and train our final model:\n\nmodel = poly_LR(deg = 1)\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('poly', PolynomialFeatures(degree=1)),\n                ('LR', LogisticRegression(max_iter=1000, penalty='none'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('poly', PolynomialFeatures(degree=1)),\n                ('LR', LogisticRegression(max_iter=1000, penalty='none'))])PolynomialFeaturesPolynomialFeatures(degree=1)LogisticRegressionLogisticRegression(max_iter=1000, penalty='none')\n\n\n\n\nAuditing the Model\nWe can now test our model agains the testing data and audit its performance. For this, we make use of the confusion matrix which will allow us to calculate the Positive Predictive Value (PPV) of our model as well as the False Positive Rate (FPR) and False Negative Rate (FNR). Recall that:\n\\[PPV = \\frac{TP}{TP + FP} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; FNR = \\frac{FN}{FN + TP} \\;\\;\\;\\;\\;\\;\\;\\;\\; FPR = \\frac{FP}{FP + TN}\\]\nLet’s define a function to perform this calculations for us:\n\nfrom sklearn.metrics import confusion_matrix\n\ndef audit_model(y_test, y_pred):\n    c_matrix = confusion_matrix(y_test, y_pred)\n    # Get values from consution matrix:\n    TN = c_matrix[0][0]\n    FP = c_matrix[0][1]\n    FN = c_matrix[1][0]\n    TP = c_matrix[1][1]\n    PPV = ( TP / (TP + FP) ).round(5)\n    FNR = ( FN / (FN + TP) ).round(5)\n    FPR = ( FP / (FP + TN) ).round(5)\n    print(f\"PPV = {PPV}, FNR = {FNR}, FPR = {FPR}\") \n\n\nOverall Measures\nWe are now able to examine the overall accuracy, as well as the values calculated above:\n\n# Get predictions \ny_hat = model.predict(X_test)\n\n# Accuracy \naccuracy = (y_hat == y_test).mean().round(5)\nprint(f\"Accuracy = {accuracy}\")\n\n# Find the other values\naudit_model(y_test, y_hat)\n\nAccuracy = 0.76848\nPPV = 0.72412, FNR = 0.29729, FPR = 0.18586\n\n\nWe observe that the model is fairly accurate with a high PPV, which means that a predictive positive has roughly 72% of being a true positive. Moreover, our model is slightly more likely to incorrectly predict that an individual with a salary less than $50K earns more than the oppostive scenario.\n\n\nBy-Group Measures\nNow, to actually evaluate if our model presents some sort of biases for any of our groups, lets analyze how the model does for each group (male and female) separately:\n\n# For male: \naccuracy_male = (y_hat == y_test)[group_test==1].mean().round(5)\nprint(\"Results for male individuals:\")\nprint(f\"Accuracy: {accuracy_male}\")\naudit_model(y_test[group_test==1], y_hat[group_test==1])\n\nprint(\"\")\n# For female: \naccuracy_female = (y_hat == y_test)[group_test==2].mean().round(5)\nprint(\"Results for female individuals:\")\nprint(f\"Accuracy: {accuracy_female}\")\naudit_model(y_test[group_test==2], y_hat[group_test==2])\n\nResults for male individuals:\nAccuracy: 0.76694\nPPV = 0.7875, FNR = 0.31429, FPR = 0.16197\n\nResults for female individuals:\nAccuracy: 0.7702\nPPV = 0.65014, FNR = 0.27176, FPR = 0.20757\n\n\nIt is evident that our model is not calibrated, given that the PPV is not the same for all the groups. According to the results above, our model is more likely to predict that a male individual earns more than $50K compared to female individuals.\nMoreover, our model also does not satisfy error rate balance. There is a clear disparity with the FNR and FPR values. The FNR value is slightly higher for male individuals, which means that male individuals are slightly more likely to be predicted a negative label when their label is actually positive. The FPR value is greater for female individuals, meaning that a female individual is more likely to be incorrectly predicted a positive label when it is actually negative.\nFinally, our model does not satisfy statistical parity given that the PPV values for both groups are different.\n\n\n\nConclusion\nThere are different individuals/companies that could benefit from a model that is able to predict an individual’s income. For instance, this model could be used by companies to strategically advertise their products to individuals based on their income. A more serious example is could be an insurer or a loaner that make decisions according to an individual’s income.\nAccording to the audit of our model above, our model seems to display some problematic biases. First, our model seems to not be calibrated. This is problematic given that our model seems to be more accurate in terms of positive labels for a group (in this case male individuals). Moreover, our model does not satisfy error rate balance, meaning that it makes different type of mistakes for different groups.\nDue to this results, our model could negatively affect some individuals. For example, if there is a government program designated to help individuals making less than $50K a year, female individuals would be more likely to incorrectly be predicted a higher salary thus not receiving aid from the government. In the example above, for insurers or loaners, male individuals would be negatively impacted as the model is slightly more likely to predict that male individuals earn less than they actually do. Overall, this model could definitely have negative impact on different groups, which represents a form of allocative bias.\nBeyond these biases, it does not really feel right to deploy a model to make important decisions such as receiving aid from the government or an important credit. It is hard for a model to take a hollistic particular approach for each individual. A calibrated model might be useful to get some initial information about an individual, but there should be poeple in charge of regulating the biases for a specific individual."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on some linear algebra methods for unsupervised learning for different kind of data\n\n\n\n\n\n\nMay 14, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on allocative bias in machine learning\n\n\n\n\n\n\nMay 2, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on linear regression and some experiments involving modifications like the LASSO Regularization\n\n\n\n\n\n\nMar 26, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the Kernel Logistic Regression\n\n\n\n\n\n\nMar 24, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the optimization for Logistic Regression based on the gradient of functions\n\n\n\n\n\n\nMar 9, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nFeb 23, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]