[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Link to Code\nhttps://github.com/eduparlema/eduparlema.github.io/blob/main/posts/kernel-logistic-regression/kernel_logistic.py\n\n\nBackground Information and Implementation\nIn kernel logistic regression we still perform Empirical Risk Minimization but with a modified loss function: \\[L_k(v) = \\frac{1}{n} \\sum_{i=1}^n l(\\langle v \\; , \\; k(x_i) \\rangle, y_i)\\] where \\(v \\in \\mathbb{R}^n\\) and \\(k(x_i)\\) is a modified feature vector dependent on a kernel function.\nIn order to implement this, we make use of the minimize function from scipy.optimize. Similarly, we start by padding \\(X\\) and initializing a random vector \\(v\\) (in this case of size \\(n\\)). Then, we compute the modified feature vector. To calculate the empirical risk we simply find the logistic loss of a matrix multiplication between the modified feature vector and \\(v\\). Note that we are able to do this since the predictor function is still and inner product.\n\nimport numpy as np \nfrom scipy.optimize import minimize\n\ndef sigmoid(self, z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\ndef fit(self, X, y):\n        self.X_train = X\n        X_ = self.pad(X)\n\n        v0 = np.random.rand(X.shape[0])\n        km = self.kernel(X_, X_, **self.kernel_kwargs)\n        \n        def empirical_risk(km, y, v, loss):\n            y_hat = km@v\n            return loss(y_hat, y).mean()\n        \n        result = minimize(lambda v: empirical_risk(km, y, v, self.logistic_loss), x0 = v0) \n        self.v = result.x\n\n\n\nBasic Checks\nIf we test our implementation, we find that the model is able to handle non-linear decision boundaries successfully. Here, the “rbf_kernel” is the kernel function and “gamma” is a parameter to that kernel function that says how wiggly the decision boundary should be. In other words, a large gamma should result in an overfitted model.\n\n%reload_ext autoreload \n%autoreload 2\n\nfrom kernel_logistic import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, as predicted before, a large gamma results in an overfitted model:\n\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment\nLets investigate which value of gamma is best for our model by plotting a graph of the training and validation score against different gamma values:\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed()\n\ndef experiment(noise, data_geometry):\n    gamma_values = 10.0**np.arange(-1, 7)\n    df = pd.DataFrame({\"gamma\": [], \"train\" : [], \"test\" : []})\n\n    for _ in range(10): #we perform 10 runs of the experiment and take the mean \n        X_train, y_train = data_geometry(100, shuffle = True, noise = noise)\n        X_test, y_test = data_geometry(100, shuffle = True, noise = noise)\n\n        for gamma in gamma_values:\n            KLR = KernelLogisticRegression(rbf_kernel, gamma = gamma)\n            KLR.fit(X_train, y_train)\n            to_add = pd.DataFrame({\"gamma\" : [gamma],\n                                \"train\" : [KLR.score(X_train, y_train)],\n                                \"test\" : [KLR.score(X_test, y_test)]})\n\n            df = pd.concat((df, to_add))\n\n\n    means = df.groupby(\"gamma\").mean().reset_index()\n\n    plt.xscale(\"log\")\n    plt.plot(means[\"gamma\"], means[\"train\"], label = \"training\")\n    plt.plot(means[\"gamma\"], means[\"test\"], label = \"validation\")\n    plt.legend()\n    labs = plt.gca().set(xlabel = \"Value of gamma\",\n                ylabel = \"Accuracy\") \n\nexperiment(0.2, make_moons)\n\n\n\n\nFrom here, we find that a gamma of around 100 is best for this particular model. Now, what if we vary the noise of the data? Intuitively, a low noise should bring the training and validation scores “closer” since the pattern of the training data would not differ significantly from the testing data. On the other hand, higher noise should result in lower accuracy and it should take bigger values of gamma for the training score to converge.\n\nexperiment(0.1, make_moons)\n\n\n\n\n\nexperiment(0.4, make_moons)\n\n\n\n\nWhile the results fit our predictions, these graphs suggest that the value of gamma for each model is still around 100. In other words, gamma is independent from the noise!\n\n\nA Different Data Pattern\nFinally we try a different pattern to see the results.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nexperiment(0.1, make_circles)\n\n\n\n\n\nexperiment(0.2, make_circles)\n\n\n\n\nHere, we still find that the best gamma value is independent of the noise! However, such gamma value is 1 in this case, which differs from the previous pattern."
  },
  {
    "objectID": "posts/perceptron /index.html",
    "href": "posts/perceptron /index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "https://github.com/eduparlema/eduparlema.github.io/blob/main/posts/perceptron%20/perceptron.py"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-1",
    "href": "posts/perceptron /index.html#experiment-1",
    "title": "The Perceptron",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we use 2d data to show that our perceptron algorithm converges to a weight vector \\(\\tilde{w}\\) describing a separating line as shown below.\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-2",
    "href": "posts/perceptron /index.html#experiment-2",
    "title": "The Perceptron",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we also use 2d data, but not linearly separable, so that the perceptron algorithm will not converge to a weight vector \\(\\tilde{w}\\) that describes a separating line. It is possible to see that in the accuracy graph, the perceptron algorihtm never reaches a perfect accuracy\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-3",
    "href": "posts/perceptron /index.html#experiment-3",
    "title": "The Perceptron",
    "section": "Experiment 3",
    "text": "Experiment 3\nIn this experiment we try to apply the perceptron algorithm in more than 2 dimensions. Here we try 5 features:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\nBased on the graph above, it does not seem that the data is linearly separable, as the accuracy does not seem to improve with the number of iterations making it unlikely that it will converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement simple gradient descent, stochastic gradient descent, and a momentum method. These are all helpful algorithms to optimize convex functions given that finding any local minimizer is equivalent to finding the global minimizer for these kind of functions. Then, we compare the performance of these three algorithms for training logistic regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-1",
    "href": "posts/logistic-regression/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we will explore how the outcome of the gradient descent changes depending on the value of the learning rate alpha.\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs=1000)\n\n#Graph the data with a separator\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.show()\n\n#Graph the gradient history \nnum_steps = len(LR.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.show()\n\n#Graph the score history \nscore_fig = plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe first see that the algorithm works pretty well. Even though the data is not linearly separable, unlike the perceptron, the gradient descent algorithm is able to accurately fit the weights to the data to find an accurate linear separator. We are also able to see how the accuracy improves until almost reaching 1. Now, lets try chaning alpha and see how it affects the gradien descent.\n\nLR_a = LogisticRegression()\nLR_a.fit(X, y, alpha = 10, max_epochs=1000)\n\nnum_steps = len(LR_a.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR_a.loss_history, label = \"gradient\")\nplt.title(\"alpha = 10\")\nplt.show()\n\nLR_A = LogisticRegression()\nLR_A.fit(X, y, alpha = 50, max_epochs=1000)\n\nnum_steps = len(LR_A.loss_history)\nloss_fig1 = plt.plot(np.arange(num_steps) + 1, LR_A.loss_history, label = \"gradient\")\nplt.title(\"alpha = 50\")\nplt.show()\n\n\n\n\n\n\n\nWe can see in the first case that the gradient descent algorithm still converges but a little slower than with a small alpha. If we increase alpha to 50, however, the algorithm clearly does not converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-2",
    "href": "posts/logistic-regression/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we will explore how the size of the batches influences how quickly the stochastic gradient descent algorithm converges.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 20, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is possible to see how the size of the batch affets the convergence. In fact, it seems that a smaller size of the batch leads to a faster convergence. Intuitively, choosing a big size for the batch defeats the purpose of stochastic gradient descent, so it makes sense that a smaller size leads to a fast convergence."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-3",
    "href": "posts/logistic-regression/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nNow, we explore how the momentum method enhances the stochastic gradient descent.\n\n#Graph stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#Graph stochastic gradient descent with momentum \nLR_m = LogisticRegression()\nLR_m.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR_m.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_m.loss_history, label = \"stochastic gradient (momentum)\")\n\nplt.loglog()\nlegend = plt.legend() \n\n\n\n\nEven though there is some noice once it reaches convergence, the momentum method clearly speeds up convergence significantly."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "LR = LinearRegression()\nLR.fit_analytical(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.0005) \n\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.show()\n\nTraining score = 0.6837\nValidation score = 0.6899\nTraining score = 0.5028\nValidation score = 0.5113\n\n\n\n\n\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnumber_features = []  \nvalidation_score = [] \ntraining_score = []\n\nLR = LinearRegression()\n\nfor p_features in range(1, n_val):\n    number_features.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit_analytical(X_train, y_train)\n    validation_score.append(LR.score(X_val, y_val))\n    training_score.append(LR.score(X_train, y_train))\n\nplt.plot(validation_score, label = \"Validation Scores\")\nplt.plot(training_score, label = \"Training Scores\")\nlabels = plt.gca().set(xlabel = \"features\", ylabel = \"Score\")\nplt.show()\n    \n\n\n\n\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\nLR = LogisticRegression()\nLR.fit(X, y)\nplot_decision_regions(X, y, clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X, y)}\")\n\n\n\n\n\n\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n#Transform X (there's probably a cleaner way to do this)\ntransformation = lambda x_1, x_2: x_1**2 + x_2**2\ncol1 = list(map(transformation, X[:, 0], X[:, 1]))\nX[:, 0] = np.array(col1)\nX[:, 1] = 1\n\nLR = LogisticRegression()\nLR.fit(X, y)\nplot_decision_regions(X, y, clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X, y)}\")\n\n\n\n\n\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n#Transform X (there's probably a cleaner way to do this)\nX = X**2\n\nLR = LogisticRegression()\nLR.fit(X, y)\nplot_decision_regions(X, y, clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X, y)}\")"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on the Kernel Logistic Regression\n\n\n\n\n\n\nMar 24, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the optimization for Logistic Regression based on the gradient of functions\n\n\n\n\n\n\nMar 9, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nFeb 23, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]