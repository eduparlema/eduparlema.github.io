[
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Link to Code\nhttps://github.com/eduparlema/eduparlema.github.io/blob/main/posts/kernel-logistic-regression/kernel_logistic.py\n\n\nBackground Information and Implementation\nIn kernel logistic regression we still perform Empirical Risk Minimization but with a modified loss function: \\[L_k(v) = \\frac{1}{n} \\sum_{i=1}^n l(\\langle v \\; , \\; k(x_i) \\rangle, y_i)\\] where \\(v \\in \\mathbb{R}^n\\) and \\(k(x_i)\\) is a modified feature vector dependent on a kernel function.\nIn order to implement this, we make use of the minimize function from scipy.optimize. Similarly, we start by padding \\(X\\) and initializing a random vector \\(v\\) (in this case of size \\(n\\)). Then, we compute the modified feature vector. To calculate the empirical risk we simply find the logistic loss of a matrix multiplication between the modified feature vector and \\(v\\). Note that we are able to do this since the predictor function is still and inner product.\n\nimport numpy as np \nfrom scipy.optimize import minimize\n\ndef sigmoid(self, z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_loss(self, y_hat, y):\n        return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\ndef fit(self, X, y):\n        self.X_train = X\n        X_ = self.pad(X)\n\n        v0 = np.random.rand(X.shape[0])\n        km = self.kernel(X_, X_, **self.kernel_kwargs)\n        \n        def empirical_risk(km, y, v, loss):\n            y_hat = km@v\n            return loss(y_hat, y).mean()\n        \n        result = minimize(lambda v: empirical_risk(km, y, v, self.logistic_loss), x0 = v0) \n        self.v = result.x\n\n\n\nBasic Checks\nIf we test our implementation, we find that the model is able to handle non-linear decision boundaries successfully. Here, the “rbf_kernel” is the kernel function and “gamma” is a parameter to that kernel function that says how wiggly the decision boundary should be. In other words, a large gamma should result in an overfitted model.\n\n%reload_ext autoreload \n%autoreload 2\n\nfrom kernel_logistic import KernelLogisticRegression \nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nHowever, as predicted before, a large gamma results in an overfitted model:\n\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt \n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\nExperiment\nLets investigate which value of gamma is best for our model by plotting a graph of the training and validation score against different gamma values:\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed()\n\ndef experiment(noise, data_geometry):\n    gamma_values = 10.0**np.arange(-1, 7)\n    df = pd.DataFrame({\"gamma\": [], \"train\" : [], \"test\" : []})\n\n    for _ in range(10): #we perform 10 runs of the experiment and take the mean \n        X_train, y_train = data_geometry(100, shuffle = True, noise = noise)\n        X_test, y_test = data_geometry(100, shuffle = True, noise = noise)\n\n        for gamma in gamma_values:\n            KLR = KernelLogisticRegression(rbf_kernel, gamma = gamma)\n            KLR.fit(X_train, y_train)\n            to_add = pd.DataFrame({\"gamma\" : [gamma],\n                                \"train\" : [KLR.score(X_train, y_train)],\n                                \"test\" : [KLR.score(X_test, y_test)]})\n\n            df = pd.concat((df, to_add))\n\n\n    means = df.groupby(\"gamma\").mean().reset_index()\n\n    plt.xscale(\"log\")\n    plt.plot(means[\"gamma\"], means[\"train\"], label = \"training\")\n    plt.plot(means[\"gamma\"], means[\"test\"], label = \"validation\")\n    plt.legend()\n    labs = plt.gca().set(xlabel = \"Value of gamma\",\n                ylabel = \"Accuracy\") \n\nexperiment(0.2, make_moons)\n\n\n\n\nFrom here, we find that a gamma of around 100 is best for this particular model. Now, what if we vary the noise of the data? Intuitively, a low noise should bring the training and validation scores “closer” since the pattern of the training data would not differ significantly from the testing data. On the other hand, higher noise should result in lower accuracy and it should take bigger values of gamma for the training score to converge.\n\nexperiment(0.1, make_moons)\n\n\n\n\n\nexperiment(0.4, make_moons)\n\n\n\n\nWhile the results fit our predictions, these graphs suggest that the value of gamma for each model is still around 100. In other words, gamma is independent from the noise!\n\n\nA Different Data Pattern\nFinally we try a different pattern to see the results.\n\nX, y = make_circles(200, shuffle = True, noise = 0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 100)\nKLR.fit(X, y)\nKLR.score(X, y)\n\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\nexperiment(0.1, make_circles)\n\n\n\n\n\nexperiment(0.2, make_circles)\n\n\n\n\nHere, we still find that the best gamma value is independent of the noise! However, such gamma value is 1 in this case, which differs from the previous pattern."
  },
  {
    "objectID": "posts/perceptron /index.html",
    "href": "posts/perceptron /index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "The goal of this blog post is to implement the perceptron, a classic binary linear classifier algorithm. We implement such algorithm in Python, try some experiments and showcase its limitations with data that is not linearly separable. Moreover, at the end we perform a complexity analysis of this algorithm."
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-1",
    "href": "posts/perceptron /index.html#experiment-1",
    "title": "The Perceptron",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we use 2d data to show that our perceptron algorithm converges to a weight vector \\(\\tilde{w}\\) describing a separating line as shown below.\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-2",
    "href": "posts/perceptron /index.html#experiment-2",
    "title": "The Perceptron",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we also use 2d data, but not linearly separable, so that the perceptron algorithm will not converge to a weight vector \\(\\tilde{w}\\) that describes a separating line. It is possible to see that in the accuracy graph, the perceptron algorihtm never reaches a perfect accuracy\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-3",
    "href": "posts/perceptron /index.html#experiment-3",
    "title": "The Perceptron",
    "section": "Experiment 3",
    "text": "Experiment 3\nIn this experiment we try to apply the perceptron algorithm in more than 2 dimensions. Here we try 5 features:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\nBased on the graph above, it does not seem that the data is linearly separable, as the accuracy does not seem to improve with the number of iterations making it unlikely that it will converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement simple gradient descent, stochastic gradient descent, and a momentum method. These are all helpful algorithms to optimize convex functions given that finding any local minimizer is equivalent to finding the global minimizer for these kind of functions. Then, we compare the performance of these three algorithms for training logistic regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-1",
    "href": "posts/logistic-regression/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we will explore how the outcome of the gradient descent changes depending on the value of the learning rate alpha.\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs=1000)\n\n#Graph the data with a separator\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.show()\n\n\n#Graph the gradient and score history \nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\nnum_steps = len(LR.loss_history)\n\naxarr[0].plot(np.arange(num_steps) + 1, LR.loss_history)\nlabs = axarr[0].set(title=\"Gradient history\", xlabel=\"Epochs\", ylabel=\"Gradient\")\naxarr[1].plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\nlabs = axarr[1].set(title=\"Accuracy history\", xlabel=\"Epochs\", ylabel=\"Accuracy\")\n\n\n\n\n\n\n\nWe first see that the algorithm works pretty well. Even though the data is not linearly separable, unlike the perceptron, the gradient descent algorithm is able to accurately fit the weights to the data to find an accurate linear separator. We are also able to see how the accuracy improves until almost reaching 1. Now, lets try chaning alpha and see how it affects the gradien descent.\n\nLR_a = LogisticRegression()\nLR_a.fit(X, y, alpha = 10, max_epochs=1000)\n\nnum_steps = len(LR_a.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR_a.loss_history)\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\nplt.title(\"alpha = 10\")\nplt.show()\n\nLR_A = LogisticRegression()\nLR_A.fit(X, y, alpha = 50, max_epochs=1000)\n\nnum_steps = len(LR_A.loss_history)\nloss_fig1 = plt.plot(np.arange(num_steps) + 1, LR_A.loss_history)\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\nplt.title(\"alpha = 50\")\nplt.show()\n\n\n\n\n\n\n\nWe can see in the first case that the gradient descent algorithm still converges but a little slower than with a small alpha. If we increase alpha to 50, however, the algorithm clearly does not converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-2",
    "href": "posts/logistic-regression/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we will explore how the size of the batches influences how quickly the stochastic gradient descent algorithm converges.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 20, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is possible to see how the size of the batch affets the convergence. In fact, it seems that a smaller size of the batch leads to a faster convergence. Intuitively, choosing a big size for the batch defeats the purpose of stochastic gradient descent, so it makes sense that a smaller size leads to a fast convergence."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-3",
    "href": "posts/logistic-regression/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nNow, we explore how the momentum method enhances the stochastic gradient descent.\n\n#Graph stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#Graph stochastic gradient descent with momentum \nLR_m = LogisticRegression()\nLR_m.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR_m.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_m.loss_history, label = \"stochastic gradient (momentum)\")\n\nplt.loglog()\nlegend = plt.legend() \n\n\n\n\nEven though there is some noice once it reaches convergence, the momentum method clearly speeds up convergence significantly."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Demo\nTo test our implementation, first let us write a function that produces data for us and let us visualize it:\n\n%reload_ext autoreload \n%autoreload 2\n\nimport numpy as np\nfrom linearRegression import LinearRegression\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\nNow, we print the scores and draw the lines from the resulting weights of both models implemented:\n\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score (Analytical) = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score (Analytical) = {LR.score(X_val, y_val).round(4)}\")\n\nLR2 = LinearRegression()\nLR2.fit_gradient(X_train, y_train, alpha = 0.0005) \n\nprint(f\"Training score (Gradient)= {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score (Gradient) = {LR2.score(X_val, y_val).round(4)}\")\n\n#Plot both analytical and gradient descent results \nx = np.linspace(0, 1, 101)\ny_analytical = (LR.w[0]*x + LR.w[1])\ny_gradient = (LR2.w[0]*x + LR2.w[1])\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\n#print lines\naxarr[0].plot(x, y_analytical, color=\"black\", label=\"analytical\")\naxarr[1].plot(x, y_analytical, color=\"black\", label=\"analytical\")\naxarr[0].plot(x, y_gradient, color=\"red\", label=\"gradient\")\naxarr[1].plot(x, y_gradient, color=\"red\", label=\"gradient\")\naxarr[0].legend(loc=\"best\")\naxarr[1].legend(loc=\"best\")\n\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\nTraining score (Analytical) = 0.6114\nValidation score (Analytical) = 0.6243\nTraining score (Gradient)= 0.5788\nValidation score (Gradient) = 0.6177\n\n\n\n\n\nWe see how both implementations lead to the similar results. The analytical formula seems to me slightly better, this is probably due to the learning rate of the gradient descent or the number of iterations.\nNow, to make sure the implementation of gradient descent is correct, let us visualize the how the score changed over time. The score should increase monotonically, and it does:\n\nplt.plot(LR2.score_history)\n\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.show()\n\n\n\n\n\n\nExperiment\nWhat if we increaase the number of features? Our model would have more data points so we should expect to produce a better training score as the number of features increase. However, would this lead to overfitting? Well, let’s perform an experiment that allow us to visualize the change in both the trainig and validation score.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nnumber_features = []  \nvalidation_score = [] \ntraining_score = []\n\nLR = LinearRegression()\n\nfor p_features in range(1, n_val):\n    number_features.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR.fit_analytical(X_train, y_train)\n    validation_score.append(LR.score(X_val, y_val))\n    training_score.append(LR.score(X_train, y_train))\n\nplt.plot(validation_score, label = \"validation\")\nplt.plot(training_score, label = \"training\")\nplt.legend(loc='best')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\nplt.show()\n    \n\n\n\n\nJust as predicted, the training score seems to approach 1 as the numer of features increase. However, the validation score clearly decreases after some point, which suggests that increasing the number of features leads to overfitting.\nWe now run the same experiment but with the LASSO algorithm. This algorithm uses a modified loss function with a regularization term \\(\\alpha\\):\n\\[L(\\vec{w}) = || Xw - y||^2_2 + \\alpha ||\\vec{w}'||_1\\]\nLet’s visualize the experiment on LASSO with varying regularization strengths:\n\n%%capture --no-display\nfrom sklearn.linear_model import Lasso\n\nnumber_features = []  \nvalidation_score_1 = [] \ntraining_score_1 = []\nvalidation_score_2 = [] \ntraining_score_2 = []\nvalidation_score_3 = [] \ntraining_score_3 = []\n\nL1 = Lasso(alpha=0.01)\nL2 = Lasso(alpha=0.001)\nL3 = Lasso(alpha=0.0001)\n\nfor p_features in range(1, n_val+5):\n    number_features.append(p_features)\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L1.fit(X_train, y_train)\n    validation_score_1.append(L1.score(X_val, y_val))\n    training_score_1.append(L1.score(X_train, y_train))\n    L2.fit(X_train, y_train)\n    validation_score_2.append(L2.score(X_val, y_val))\n    training_score_2.append(L2.score(X_train, y_train))\n    L3.fit(X_train, y_train)\n    validation_score_3.append(L3.score(X_val, y_val))\n    training_score_3.append(L3.score(X_train, y_train))\n\nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True)\n\naxarr[0].plot(validation_score_1, label = \"validation\")\naxarr[0].plot(training_score_1, label = \"training\")\naxarr[0].legend(loc='best')\n\naxarr[1].plot(validation_score_2, label = \"validation\")\naxarr[1].plot(training_score_2, label = \"training\")\naxarr[1].legend(loc='best')\n\naxarr[2].plot(validation_score_3, label = \"validation\")\naxarr[2].plot(training_score_3, label = \"training\")\naxarr[2].legend(loc='best')\n\nlabs = axarr[0].set(title = \"alpha = 0.01\", xlabel = \"Features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"alpha = 0.001\", xlabel = \"Features\")\nlabs = axarr[2].set(title = \"alpha = 0.0001\", xlabel = \"Features\")\nfig.set_size_inches(9, 4, forward=True)\n\n\n\n\nWe observe that the LASSO algorithm reaches a better accuracy than the regular linear regression, as it quickly gets close to 1. However, we observe the same overfitting trend. The regularization strenght seems to affect how the validation score drops, a smaller alpha seems to be better for overfitted models."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/learning-from-drGebru/index.html",
    "href": "posts/learning-from-drGebru/index.html",
    "title": "Learning from Dr. Timnit Gebru",
    "section": "",
    "text": "Dr. Gebru’s Talk “at” Middlebury\nIn the evening of April 24th, Dr. Gebru gave a virtual talk at Middlebury College on “Eugenics and the Promise of Utopia through Artificial General Intelligence”. In this talk Dr. Gebru raised many interesting points. She first began by showing some claims that we cannot even grasph the limittless potentials of AGI (Artificial General Intelligence). Nowadays, with rapid-growing technologies such as Chat-GPT or DALL-E, such claims have become popular. However, why do we want to develop AGI?\nAccording to Dr. Gebru’s talk, the need for developing AGI goes way back to the 20th century with eugenics. The first-wave of eugenics was the idea that we can improve the “human stock” through genetics and heredity. Although many people related the term eugenics with the tragedies that took place during WW2, there has been a second-wave of eugenics through the TESCREAL ideologies. These ideologies are rooted in the idea of “transhumanism”, the idea of not simply improving the “human stock”, but of transcending humanity.\nThe TESCREAL ideologies are deeply realted with AGI, given that according to such ideologies its development promises a utopia in which “humans will merge with machines” to transcend our biological limitations. However, as Dr. Gebru points out, here lies the biggest problem. AGI might promise a utopia, but for whom? Maybe in a few years people will have access to bots such as Chat-GPT to inquire about medical concerns or have access to many other services for a much lower cost. But, who is benefitting from this? Such bots are not paying any certified medical profession nor any people whose data is being used to train such models. It seems like the ones who are benefitting are the big companies who have the power to develop and train such bots. We do not even need to imagine a possible future, nowadays there have been reports of people in developing countries who are being exploited to monitor the toxic responses of chatbots or images from generative art AI.\nI totally agree with Dr. Gebru’s discussion. People in who are in charge of controlling and developing new technologies will keep getting more powerful and richer. However, I wish she went a little further into considering this scenario. There have always been instances in history in which a new development benefited some privileged individuals at first and then went on becoming available to most of the population. I think that AGI is a very particular case because I fear that in this case there will not be a switch when everyone could benefit from it. Depending on the direction this takes, AGI might be able to replace most of people’s jobs while benefitting only big AI companies. But this time, these AI companies will be much smaller compared to the wide population. At some point a limit must be reached, a very small part of the population cannot keep benefiting from this while the vast majority is not.\nThe interaction with Dr. Gebru was empowering. The idea of AGI is frightening in itself, but Dr. Gebru is an example of how the development of such technologies could actually take an interesting and ethical direction. We are living exciting times and we as potential future developers or researchers must bear in mind that there could be negative consequences as result of our work. Dr. Gebru definitely inspired me to continue investingating this exciting area of computer science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A reflective blog post on Dr. Timnit Gebru’s talk at Middlebury\n\n\n\n\n\n\nApr 19, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on linear regression and some experiments involving modifications like the LASSO Regularization\n\n\n\n\n\n\nMar 26, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the Kernel Logistic Regression\n\n\n\n\n\n\nMar 24, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the optimization for Logistic Regression based on the gradient of functions\n\n\n\n\n\n\nMar 9, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nFeb 23, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]