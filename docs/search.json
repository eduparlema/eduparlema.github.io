[
  {
    "objectID": "posts/perceptron /index.html",
    "href": "posts/perceptron /index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "https://github.com/eduparlema/eduparlema.github.io/blob/main/posts/perceptron%20/perceptron.py"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-1",
    "href": "posts/perceptron /index.html#experiment-1",
    "title": "The Perceptron",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we use 2d data to show that our perceptron algorithm converges to a weight vector \\(\\tilde{w}\\) describing a separating line as shown below.\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-2",
    "href": "posts/perceptron /index.html#experiment-2",
    "title": "The Perceptron",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we also use 2d data, but not linearly separable, so that the perceptron algorithm will not converge to a weight vector \\(\\tilde{w}\\) that describes a separating line. It is possible to see that in the accuracy graph, the perceptron algorihtm never reaches a perfect accuracy\n\nn = 100 \np_features = 3 \n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0, 0), (0, 0)])\n\np = Perceptron() \np.fit(X, y, max_steps = 1000)\n\n\ndata_fig = plt.scatter(X[:,0], X[:,1], c = y)\ndata_fig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nplt.title(\"Data with Separating Line\")\n\nplt.show()\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\nplt.title(\"Accuracy over Training\")\n\nplt.show()"
  },
  {
    "objectID": "posts/perceptron /index.html#experiment-3",
    "href": "posts/perceptron /index.html#experiment-3",
    "title": "The Perceptron",
    "section": "Experiment 3",
    "text": "Experiment 3\nIn this experiment we try to apply the perceptron algorithm in more than 2 dimensions. Here we try 5 features:\n\nn = 100 \np_features = 5\n\nX, y = make_blobs(n_samples = 100, n_features = p_features, centers = np.random.uniform(-1, 1, (2, p_features)))\n\n\np = Perceptron()\np.fit(X,y, max_steps = 1000)\n\naccuracy_fig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over iterations\")\n\nplt.show()\n\n\n\n\nBased on the graph above, it does not seem that the data is linearly separable, as the accuracy does not seem to improve with the number of iterations making it unlikely that it will converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement simple gradient descent, stochastic gradient descent, and a momentum method. These are all helpful algorithms to optimize convex functions given that finding any local minimizer is equivalent to finding the global minimizer for these kind of functions. Then, we compare the performance of these three algorithms for training logistic regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-1",
    "href": "posts/logistic-regression/index.html#experiment-1",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 1",
    "text": "Experiment 1\nIn this experiment we will explore how the outcome of the gradient descent changes depending on the value of the learning rate alpha.\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.5, max_epochs=1000)\n\n#Graph the data with a separator\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.show()\n\n#Graph the gradient history \nnum_steps = len(LR.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.show()\n\n#Graph the score history \nscore_fig = plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe first see that the algorithm works pretty well. Even though the data is not linearly separable, unlike the perceptron, the gradient descent algorithm is able to accurately fit the weights to the data to find an accurate linear separator. We are also able to see how the accuracy improves until almost reaching 1. Now, lets try chaning alpha and see how it affects the gradien descent.\n\nLR_a = LogisticRegression()\nLR_a.fit(X, y, alpha = 10, max_epochs=1000)\n\nnum_steps = len(LR_a.loss_history)\nloss_fig = plt.plot(np.arange(num_steps) + 1, LR_a.loss_history, label = \"gradient\")\nplt.title(\"alpha = 10\")\nplt.show()\n\nLR_A = LogisticRegression()\nLR_A.fit(X, y, alpha = 50, max_epochs=1000)\n\nnum_steps = len(LR_A.loss_history)\nloss_fig1 = plt.plot(np.arange(num_steps) + 1, LR_A.loss_history, label = \"gradient\")\nplt.title(\"alpha = 50\")\nplt.show()\n\n\n\n\n\n\n\nWe can see in the first case that the gradient descent algorithm still converges but a little slower than with a small alpha. If we increase alpha to 50, however, the algorithm clearly does not converge."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-2",
    "href": "posts/logistic-regression/index.html#experiment-2",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 2",
    "text": "Experiment 2\nIn this experiment we will explore how the size of the batches influences how quickly the stochastic gradient descent algorithm converges.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 10\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    batch_size = 20, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch_size = 20\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nIt is possible to see how the size of the batch affets the convergence. In fact, it seems that a smaller size of the batch leads to a faster convergence. Intuitively, choosing a big size for the batch defeats the purpose of stochastic gradient descent, so it makes sense that a smaller size leads to a fast convergence."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiment-3",
    "href": "posts/logistic-regression/index.html#experiment-3",
    "title": "Optimization for Logistic Regression",
    "section": "Experiment 3",
    "text": "Experiment 3\nNow, we explore how the momentum method enhances the stochastic gradient descent.\n\n#Graph stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = False, \n    batch_size = 10, \n    alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n#Graph stochastic gradient descent with momentum \nLR_m = LogisticRegression()\nLR_m.fit_stochastic(X, y, \n    max_epochs = 1000, \n    momentum = True, \n    batch_size = 10, \n    alpha = 0.1)\n\nnum_steps = len(LR_m.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_m.loss_history, label = \"stochastic gradient (momentum)\")\n\nplt.loglog()\nlegend = plt.legend() \n\n\n\n\nEven though there is some noice once it reaches convergence, the momentum method clearly speeds up convergence significantly."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post on the optimization for Logistic Regression based on the gradient of functions\n\n\n\n\n\n\nMar 9, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post on the perceptron algorithm\n\n\n\n\n\n\nFeb 23, 2023\n\n\nEduardo Pareja Lema\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques youâ€™ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]