<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Eduardo Pareja Lema, Ian Brons">
<meta name="dcterms.date" content="2023-05-15">
<meta name="description" content="A blog post on the final project of CS 0451">

<title>My Awesome CSCI 0451 Blog - Project Blog Post</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
    }
    </style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Project Blog Post</h1>
                  <div>
        <div class="description">
          A blog post on the final project of CS 0451
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Eduardo Pareja Lema, Ian Brons </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 15, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>For our final project we decided to create an agent that learns how to play the strategy board game Risk. For this, we explore a novel area of machine learning, namely, reinforcement learning. The greates challenge of the project was to create an appropiate environment that both simulates Risk, and it is also suitable environment for reinforcement learning. That is, for every action we take on the environment, we must be able to return a reward and an observation that represents the current state of the environment. At the end of the semester, we managed to design an implement a fully functional Risk environment that follows OpenAI Gymnaium’s API as well as visualization script that shows the progress of the game. We also created a DQN agent using deep reinforcement learning and we are currently in the training phase. We are still learning about deep reinforcement learning and we believe we will be able to fine tune the training of the agent in the future.</p>
<p>Here is the link to our repository:</p>
<p><a href="https://github.com/EdBrons/risk" class="uri">https://github.com/EdBrons/risk</a></p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Our ultimate goal was to create an agent that learns how to play the strategy board game Risk. Over the years, artificial intelligence for board games has been an active field of research for different rearons. For one, they represent a benchmark for evaluating the performance of AI algorithms. Moreover, given that board games are a popular form of entertainment this facilitates human-machine interactions. And finally, AI for board games are an attractive introduction for advanced areas of machine learning such as deep reinforcement learning. A very popular example that encapsulates all of the above is the AlphaZero, an agent created by DeepMind that masters ches <span class="citation" data-cites="silver2017mastering">Silver et al. (<a href="#ref-silver2017mastering" role="doc-biblioref">2017</a>)</span>.</p>
<p>We used reinforcement learning for the project and to get some background knowledge we explored “Reinforcement Learning” by Barto and Sutton <span class="citation" data-cites="sutton1999reinforcement">Sutton, Barto, et al. (<a href="#ref-sutton1999reinforcement" role="doc-biblioref">1999</a>)</span>. Finally, there have been previous attempts to train an agent to play Risk, for example <span class="citation" data-cites="Wolf2005AnIA">Wolf (<a href="#ref-Wolf2005AnIA" role="doc-biblioref">2005</a>)</span>. However, in their approach, a TD algorithm was used. We chose to use a Q-learning algorithm given that Risk is a complex strategy game and Q-learning incorporates a exploration-explotation strategy (typically using epsilon-greedy).</p>
</section>
<section id="values-statement" class="level1">
<h1>Values Statement</h1>
<p>The potential users are board game enthusiasts who are interested in improving their skills at playing Risk or discover new strategies followed by the AI. Our project could also potentially have some impacts on the AI for boadgames community as it could provide some insights for similar implementations. Besides these, we do not see any direct benefit nor harm that our agent could potentially cause.</p>
<p>My personal reason to work on this project was my interest in the theory behind Reinforcement Learning. I believe that the project gave me a lot of opportunities to both learn some theory and mainly practice my coding skills in designing the Risk environment.</p>
</section>
<section id="materials-and-methods" class="level1">
<h1>Materials and Methods</h1>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>For our project we did not need to collect any data directly. Most of the project involved creating a working Risk environment that simulates the game. Once we achieved this, data is created itself during the training phase. For instance, the environment is designed such that after every action taken by the AI, we return an observation and a reward that is later used to take other decisions by the agent.</p>
</section>
<section id="approach" class="level3">
<h3 class="anchored" data-anchor-id="approach">Approach</h3>
<p>We had two main challenges for the project: The Risk environment and the Reinforcement Learning algorithm.</p>
<p>Risk is a very complex board game that has a very large state space. That is, there are a lot of possible configurations of the board. In order to have a suitable environment, we decided to follow OpenAI’s Gymnasium API. That is, our environment must have an <code>action_space</code> and <code>observation_space</code> attributes as well as the the following methods: step, reset, render and close. The most important method is step, which takes as input an action, and returns a reward, an observation, and whether the game has terminated or not. This represented a big challenge given that risk has different phases during a single turn. First, a player gets some armies according to the number of territories they own, then these can be placed in any of the territories they own. Then comes the attack phase, where the player must choose where to attack. Finally, depending on the result of the attack, the player may choose to attack again or simply reinforce from neighboring territories. This poses a challenge given that at different stages of the game, different actions are possible. For instance, if a player wins an attack they can only choose between two options, whether to attack or not, but before attacking, the player can choose to attack to any territory they do not own. Unfortunately, OpenAI’s Gymnasium indicates that the <code>action_space</code> must be one of their data strucures (or Spaces), and these are fixed.</p>
<p>To solve this issue, we decided to have a Discrete action space of 43 possible actions (one for each territory in risk and one extra to just do nothing). Then, we divided one single turn of risk into several phases. Of course, the main issue with this is that sometimes the agent might choose a wrong action. For instance, if it chooses to attack its own territory. We tried to fix this by giving the agent a negative reward if it chooses an invalid move and we also passed all its possible valid moves at a given state through the observation step. From our training, it seems that the agent is able to learn to choose the correct moves as the reward increases over time as we train the agent.</p>
<p>The second issue was the Reinforcement Learning algorithm. From the research we did, we decided it was best to use the <code>keras-rl</code> package which is essentially a deep reinforcement learning algorithm. Moreover, we chose to create a <code>DQNAgent</code> mainly because its exploration-exploitation policy and given its flexibility to design a simple reward function. We had a big issue with the <code>observation_space</code> given that we wanted to pass the agent both an observation of the environment after a step and also the current phase it is at as well as the current valid moves, so that it would learn to not make invalid moves. However, this represented a problem as most of the examples out there that uses this algorithm use a simple <code>observation_space</code>. Our observation space was a dictionary and so to fix the issue we wrote our own simple Processor given that the <code>MultiInputProcessor</code> from the <code>keras-rl</code> package was not really working for our particular case.</p>
<p>At the end we were able to start training the agent against random players but we run out of time to actually see the results of our agent. Given the complexity of Risk, our environment and mainly our <code>observation_space</code>, we believe that training our agent will require a lot of computing power and time. We plan to first test the efficienty of our agent against random players and eventually we were planning to train the agent isung self-play which would definitely take more computing power but we believe would result in a much better agent.</p>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="out.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Training Gif</figcaption><p></p>
</figure>
</div>
<p>The gif above showcases our agent training. Unfortunately we ran out of time for the semester but according to the little training we were able to do, the reward seemed to increase slightly showing signs that our agent was learning to choose the right moves at each phase. Our biggest result is the fully-functional risk environment that both correctly emulates Risk for a given number of players (between 2 and 6), and correctly follows OpenAI’s Gymnasium API. Other reinforcement learners could use our environment to create better risk agents. Also we created a visualization tool so that they can visualize how their agents play the game.</p>
<p>Moreover, our work gives a lot of different paths for future work. For example, we could simplfy dramatically the <code>action_space</code> of our algorithm to only allow the agent to choose between yes or no, and in the step function we could implement some predetermined strategies that the agent would be able to easily learn.</p>
<p>One big factor holding us back is our lack of knowledge in deep reinforcement learning. Once we have a better grasp of this complex area, we will be able to fine tune our algorithm.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Our project worked in the sense that we were able to design and implement a Risk environment in which different Reinforcement Learning algorithms can be applied to. Also, we were able to implement a functional <code>DQNAgent</code> that is trainable on our environment, we name it Noel Gallagher after Noel Gallagher the chief songwriter and lead guatarist of the hit brit pop band Oasis, who is widely speculated to be a Risk enthusiast.</p>
<p>Unfortunately, we did not meet all the goals we set at the beginning of the project. We were hoping to have a trained agent by the end of it but we were not able to achieve so. However, we did fulfill our goals in terms of exploring reinforcement learning and even grasping the more complex area of deep reinforcement learning. If we had more time and computing power, we believe we could have achieved a trained agent. Moreover, with additional time we could have explored different deep reinforcement learning algorithms that could potentially work better for our environment. We are generally happy with our results given that we did not have an extensive period of time to work on this complext project.</p>
</section>
<section id="group-contribution" class="level1">
<h1>Group Contribution</h1>
<p>We mostly worked together (phisically) in most parts of the project, specially on the environment and the <code>keras-rl</code> algorithm implementation. Ian worked on the visualization of the environment.</p>
</section>
<section id="personal-reflection" class="level1">
<h1>Personal Reflection</h1>
<p>I found this project to be very rewarding. Even though, we did not fully achieve our initial goals, the process of planning and implementing the project helped me grow a lot. First, this was my first time setting up a git work-flow and planning a technical project so I really enjoyed that aspect. Moreover, before starting with the project, we spent some time learning about Reinforcement Learning and discussing ideas on how to implement the environment. I came across a variety of new ideas and I found myself comming up with interesting solution for problems we found along the way.</p>
<p>Overall, I learned new theory on Reinforcement Learning, specially on Q-learning and PPO algorithms and some ways these are implemented. Moreover, I became aware of the fascinating area of research devoted to develop AI agents to play board games/video games, as well as OpenAI’s Gymnaium, an amazing place for those getting started with RL. I believe that one of the most valuable skills I gained during the project was communication. We maintained a very efficient git work-flow that gave room for both of us to try new things and report back on things that worked and things that did not.</p>
<p>Even though it might seem we did not achieve a lot, I feel we did achieve a lot. I was not only able to explore a completely new area of machine learning, but I also really improved my coding and design skills. Designing the environment was a really complext task and I feel we achieved a lot by having a fully functional and trainable environment that simulates such a complex board game as Risk. I will carry my experience in this project in many ways. Now I feel much more prepared and confident to start working on project on my own and specially I feel a lot more comfortable working and planning a project with others. Finally, the experience I gained going through documentations helpe gain valuable experience in terms of learning how to solve problems and specially learning how to go from theory to implementation. I am happy with the results and proud of what we achieved.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-silver2017mastering" class="csl-entry" role="doc-biblioentry">
Silver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2017. <span>“Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.”</span> <em>arXiv Preprint arXiv:1712.01815</em>.
</div>
<div id="ref-sutton1999reinforcement" class="csl-entry" role="doc-biblioentry">
Sutton, Richard S, Andrew G Barto, et al. 1999. <span>“Reinforcement Learning.”</span> <em>Journal of Cognitive Neuroscience</em> 11 (1): 126–34.
</div>
<div id="ref-Wolf2005AnIA" class="csl-entry" role="doc-biblioentry">
Wolf, M. Muñoz. 2005. <span>“An Intelligent Artificial Player for the Game of Risk.”</span> In.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>